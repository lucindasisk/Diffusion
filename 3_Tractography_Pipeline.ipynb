{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nipype.interfaces.io import DataSink, SelectFiles, DataGrabber \n",
    "from nipype.interfaces.utility import IdentityInterface, Function    \n",
    "from nipype.pipeline.engine import Node, Workflow, JoinNode, MapNode\n",
    "import nipype.interfaces.mrtrix3 as mtx\n",
    "import nipype.interfaces.mrtrix.convert as mtxc\n",
    "import nipype.interfaces.mrtrix.preprocess as mtxp\n",
    "import nipype.interfaces.fsl as fsl\n",
    "from pandas import Series, read_csv, to_numeric\n",
    "from glob import glob\n",
    "from os.path import abspath, expanduser, join\n",
    "from os import chdir, remove, getcwd, makedirs\n",
    "from shutil import copyfile\n",
    "from nipype import config, logging\n",
    "from datetime import date\n",
    "today = str(date.today())\n",
    "config.enable_debug_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set user and path variables\n",
    "local='False'\n",
    "user = expanduser('~')\n",
    "if user == '/Users/lucindasisk':\n",
    "    if local == 'True':\n",
    "        laptop = '/Users/lucindasisk/Desktop/DATA'\n",
    "        home = join(user, 'Desktop/Milgram/candlab')\n",
    "        raw_dir = join(home, 'data/mri/bids_recon/shapes')\n",
    "        proc_dir = join(home, 'analyses/shapes/dwi')\n",
    "        workflow_dir = join(laptop, 'workflows_ls')\n",
    "        data_dir = join(laptop, 'data_ls')\n",
    "    else:\n",
    "        home = join(user, 'Desktop/Milgram/candlab')\n",
    "        raw_dir = join(home, 'data/mri/bids_recon/shapes')\n",
    "        proc_dir = join(home, 'analyses/shapes/dwi/data')\n",
    "        workflow_dir = join(home, 'analyses/shapes/dwi/workflows')\n",
    "        data_dir = join(home, 'analyses/shapes/dwi/data')\n",
    "else:\n",
    "    home = '/gpfs/milgram/project/gee_dylan/candlab'\n",
    "    raw_dir = join(home, 'data/mri/bids_recon/shapes')\n",
    "    proc_dir = join(home, 'analyses/shapes/dwi/data')\n",
    "    workflow_dir = join(home, 'analyses/shapes/dwi/workflows')\n",
    "    data_dir = join(home, 'analyses/shapes/dwi/data')\n",
    "    \n",
    "# Read in subject subject_list\n",
    "subject_info = read_csv(\n",
    "    home + '/scripts/shapes/mri/dwi/shapes_dwi_subjList_08.07.2019.txt', sep=' ', header=None)\n",
    "subject_list = subject_info[0].tolist()\n",
    "\n",
    "# Manual subject list\n",
    "subject_list = ['sub-A208', 'sub-A207']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Datasink, Infosource, Selectfiles\n",
    "\n",
    "datasink = Node(DataSink(base_directory = data_dir,\n",
    "                        substitutions = [('_subject_id_', '')]),\n",
    "                   name='datasink')\n",
    "\n",
    "#Set infosource iterables\n",
    "infosource = Node(IdentityInterface(fields=['subject_id']),\n",
    "                  name=\"infosource\")\n",
    "infosource.iterables = [('subject_id', subject_list)]\n",
    "\n",
    "#SelectFiles\n",
    "template = dict(dti = join(proc_dir,'3_Eddy_Corrected/{subject_id}/eddy_corrected_resample.nii.gz'),\n",
    "                bval = join(raw_dir, '{subject_id}/ses-shapesV1/dwi/{subject_id}_ses-shapesV1_dwi.bval'),\n",
    "                bvec = join(proc_dir,'3_Eddy_Corrected/{subject_id}/eddy_corrected.eddy_rotated_bvecs'),\n",
    "                t1 = join(proc_dir, '2_Preprocessed/{subject_id}/{subject_id}_ses-shapesV1_T1w_flirt_brain.nii.gz'),\n",
    "                aseg = join(home, 'data/mri/hcp_pipeline_preproc/shapes/{subject_id}/MNINonLinear/aparc.a2009s+aseg.nii.gz'),\n",
    "                mni=join(home, 'atlases/MNI152_T1_2mm_brain.nii.gz')\n",
    "               )\n",
    "\n",
    "sf = Node(SelectFiles(template, \n",
    "                      base_directory = home),\n",
    "          name = 'sf')\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes for Diffusion workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate binary mask\n",
    "bet=Node(fsl.BET(frac=0.2,\n",
    "                mask=True),\n",
    "        name='bet')\n",
    "\n",
    "#Convert bvals and bvecs to fslgrad\n",
    "gradconv = Node(mtx.MRConvert(out_file = 'dwi_converted.mif'),\n",
    "               name = 'gradconv')\n",
    "\n",
    "#Generate 5 tissue type (5tt) segmentation using FAST algorithm\n",
    "seg5tt = Node(mtx.Generate5tt(algorithm = 'fsl',\n",
    "                             out_file = 'T1s_5tt_segmented.nii.gz'),\n",
    "             name='seg5tt')\n",
    "\n",
    "#Estimate response functions for spherical deconvolution using the specified algorithm (Dhollander)\n",
    "#https://nipype.readthedocs.io/en/latest/interfaces/generated/interfaces.mrtrix3/preprocess.html#responsesd\n",
    "#https://mrtrix.readthedocs.io/en/latest/constrained_spherical_deconvolution/response_function_estimation.html#response-function-estimation\n",
    "#Max_sh (lmax variable) determined in shell order from here: https://mrtrix.readthedocs.io/en/3.0_rc2/constrained_spherical_deconvolution/lmax.html\n",
    "#DWI has 5 shells: 7 b0 volumes, 6 b500 vols, 15 b1000 vols, 15 b2000 bols, 60 b3000 vols\n",
    "dwiresp = Node(mtx.ResponseSD(algorithm = 'dhollander',\n",
    "                              max_sh=[0,2,4,4,8],\n",
    "                              wm_file = 'wm_response.txt',\n",
    "                              gm_file = 'gm_response.txt',\n",
    "                              csf_file = 'csf_response.txt'),\n",
    "              name='dwiresp')\n",
    "\n",
    "#Estimate fiber orientation distributions from diffusion data sing spherical deconvolution\n",
    "#https://nipype.readthedocs.io/en/latest/interfaces/generated/interfaces.mrtrix3/reconst.html\n",
    "#https://mrtrix.readthedocs.io/en/latest/constrained_spherical_deconvolution/multi_shell_multi_tissue_csd.html\n",
    "#Max SH here determined by tissue type - chose 8,8,8 per forum recommendations\n",
    "mscsd = Node(mtx.EstimateFOD(algorithm = 'msmt_csd',\n",
    "                             bval_scale = 'yes',\n",
    "                            max_sh = [8,8,8]),\n",
    "            name='mscsd')\n",
    "\n",
    "#Perform Tractography - ACT using iFOD2 (https://nipype.readthedocs.io/en/latest/interfaces/generated/interfaces.mrtrix3/tracking.html) \n",
    "tract = Node(mtx.Tractography(algorithm='iFOD2',\n",
    "                              select=100000, #Jiook has done 100 million streamlines\n",
    "                              n_trials=10000, \n",
    "                              out_file='whole_brain_tracktography.tck'),\n",
    "            name='tract')\n",
    "\n",
    "#Convert whole-brain tractography from MrTrix format to TrackVis\n",
    "trkconvert = Node(mtxc.MRTrix2TrackVis(out_filename = 'whole_brain_tractography_converted.trk'),\n",
    "                 name='trkconvert')\n",
    "\n",
    "#convert eddy-corrected raw DTI to tensor format\n",
    "dwi2tensor = Node(mtx.FitTensor(out_file = 'whole_brain_tensorfile.mif',\n",
    "                               bval_scale='yes'),\n",
    "                name='dwi2tensor')\n",
    "\n",
    "#Compute FA from tensor files\n",
    "tensor2fa = Node(mtx.TensorMetrics(out_fa='whole_brain_FA.mif'),\n",
    "                name='tensor2fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_flow = Workflow(name = 'tract_flow')\n",
    "tract_flow.connect([(infosource, sf, [('subject_id','subject_id')]),\n",
    "                    #Skullstrip T1\n",
    "                    (sf, bet, [('t1', 'in_file')]),\n",
    "#                     #Segment T1 image with FSL 5tt algorithm\n",
    "#                     (sf, seg5tt, [('t1', 'in_file')]),\n",
    "#                     (seg5tt, datasink, [('out_file', '5_tract_Reconstruction')]),\n",
    "                    #Convert bval/bvec to gradient tables\n",
    "                    (sf, gradconv, [('dti', 'in_file'),\n",
    "                                   ('bval','in_bval'),\n",
    "                                   ('bvec', 'in_bvec')]),\n",
    "                    #Compute FOD response functions\n",
    "                    (gradconv, dwiresp, [('out_file', 'in_file')]),\n",
    "                    (dwiresp, datasink, [('wm_file', '5_tract_Reconstruction.@par'),\n",
    "                                        ('gm_file', '5_tract_Reconstruction.@par.@par'),\n",
    "                                        ('csf_file', '5_tract_Reconstruction.@par.@par.@par')]),\n",
    "                    (sf, mscsd, [('dti', 'in_file'),\n",
    "                                ('bval', 'in_bval'),\n",
    "                                ('bvec', 'in_bvec')]),\n",
    "                    #Perform multi-shell constrained spherical deconvolution\n",
    "                    (dwiresp, mscsd, [('wm_file', 'wm_txt'),\n",
    "                                      ('gm_file', 'gm_txt'),\n",
    "                                      ('csf_file', 'csf_txt')]),\n",
    "                    (mscsd, tract, [('wm_odf', 'in_file')]),\n",
    "                    (mscsd, datasink, [('wm_odf', '5_tract_Reconstruction.@par.@par.@par.@par'),\n",
    "                                       ('gm_odf', '5_tract_Reconstruction.@par.@par.@par.@par.@par'),\n",
    "                                       ('csf_odf','5_tract_Reconstruction.@par.@par.@par.@par.@par.@par')]),\n",
    "                    (sf, tract, [('bval', 'in_bval'),\n",
    "                                 ('bvec', 'in_bvec')]),\n",
    "                    (bet, tract, [('mask_file', 'seed_image')]),\n",
    "                    #Convert ms-csd files to global tractography\n",
    "                    (tract, trkconvert, [('out_file', 'in_file')]),\n",
    "                    (sf, trkconvert, [('t1','image_file')]),\n",
    "                    (sf, trkconvert, [('t1','registration_image_file')]),\n",
    "                    (trkconvert, datasink, [('out_file', '5_tract_Reconstruction.@par.@par.@par.@par.@par.@par.@par')]),\n",
    "                    (tract, datasink, [('out_file', '5_tract_Reconstruction.@par.@par.@par.@par.@par.@par.@par.@par')]),      \n",
    "                    (bet, datasink, [('mask_file','5_tract_Reconstruction.@par.@par.@par.@par.@par.@par.@par.@par.@par')]),\n",
    "                    #Nodes to create tensor FA files\n",
    "                    (gradconv, dwi2tensor, [('out_file', 'in_file')]),\n",
    "                    (dwi2tensor, datasink, [('out_file', '6_Tensor_Data')]),\n",
    "                    (dwi2tensor, tensor2fa, [('out_file', 'in_file')]),\n",
    "                    (tensor2fa, datasink, [('out_fa', '6_Tensor_Data.@par')]),\n",
    "                   ])\n",
    "tract_flow.base_dir = workflow_dir\n",
    "tract_flow.write_graph(graph2use = 'flat')\n",
    "dwi = tract_flow.run('MultiProc', plugin_args={'n_procs': 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os.path as op\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import nibabel as nib\n",
    "# import dipy.data as dpd\n",
    "# from dipy.data import fetcher\n",
    "# import dipy.tracking.utils as dtu\n",
    "# import dipy.tracking.streamline as dts\n",
    "# from dipy.io.streamline import save_tractogram, load_tractogram\n",
    "# from dipy.stats.analysis import afq_profile, gaussian_weights\n",
    "# from dipy.io.stateful_tractogram import StatefulTractogram\n",
    "# from dipy.io.stateful_tractogram import Space\n",
    "\n",
    "# import AFQ.utils.streamlines as aus\n",
    "# import AFQ.data as afd\n",
    "# import AFQ.tractography as aft\n",
    "# import AFQ.registration as reg\n",
    "# import AFQ.dti as dti\n",
    "# import AFQ.segmentation as seg\n",
    "# from AFQ.utils.volume import patch_up_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Nodes for AFQ Tractography Workflow\n",
    "# Load processed data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin = '/Users/lucindasisk/Desktop/DATA/tractography_data/4_tract_Reconstruction/sub-A200'\n",
    "# bval = join(raw_dir, 'sub-A200/ses-shapesV1/dwi/sub-A200_ses-shapesV1_dwi.bval')\n",
    "# bvec = join(proc_dir,'eddyCUDA_data/3_Eddy_Corrected/sub-A200/eddy_corrected.eddy_rotated_bvecs')\n",
    "# dtifile = join(proc_dir,'eddyCUDA_data/3_Eddy_Corrected/sub-A200/eddy_corrected.nii.gz')\n",
    "# img = nib.load(proc_dir +'/eddyCUDA_data/3_Eddy_Corrected/sub-A200/eddy_corrected.nii.gz')\n",
    "\n",
    "# print(\"Calculating DTI...\")\n",
    "# if not op.exists(origin + '/dti_FA.nii.gz'):\n",
    "#     dti_params = dti.fit_dti(dtifile, bval, bvec,\n",
    "#                              out_dir=origin)\n",
    "# else:\n",
    "#     dti_params = {'FA': './dti_FA.nii.gz',\n",
    "#                   'params': './dti_params.nii.gz'}\n",
    "# FA_img = nib.load(dti_params['FA'])\n",
    "# FA_data = FA_img.get_fdata()\n",
    "\n",
    "# tg = load_tractogram(origin + '/whole_brain_tractography_converted.trk', img)\n",
    "# streamlines = tg.streamlines\n",
    "\n",
    "# # streamlines = dts.Streamlines(\n",
    "# #     dtu.transform_tracking_output(streamlines,\n",
    "# #                                   np.linalg.inv(img.affine)))\n",
    "\n",
    "# templates = afd.read_templates()\n",
    "# bundle_names = [\"UNC\", \"CGC\"]\n",
    "\n",
    "# bundles = {}\n",
    "# for name in bundle_names:\n",
    "#     for hemi in ['_R', '_L']:\n",
    "#         bundles[name + hemi] = {\n",
    "#             'ROIs': [templates[name + '_roi1' + hemi],\n",
    "#                      templates[name + '_roi2' + hemi]],\n",
    "#             'rules': [True, True],\n",
    "#             'prob_map': templates[name + hemi + '_prob_map'],\n",
    "#             'cross_midline': False}\n",
    "\n",
    "# print(\"Registering to template...\")\n",
    "# MNI_T2_img = dpd.read_mni_template()\n",
    "# if not op.exists('mapping.nii.gz'):\n",
    "#     import dipy.core.gradients as dpg\n",
    "#     gtab = dpg.gradient_table(bval, bvec)\n",
    "#     warped_hardi, mapping = reg.syn_register_dwi(dtifile, gtab)\n",
    "#     reg.write_mapping(mapping, './mapping.nii.gz')\n",
    "# else:\n",
    "#     mapping = reg.read_mapping('./mapping.nii.gz', img, MNI_T2_img)\n",
    "\n",
    "# tg = load_tractogram('/Users/lucindasisk/Desktop/DATA/tractography_data/4_tract_Reconstruction/sub-A200/whole_brain_tractography_converted.trk', img)     \n",
    "# streamlines = tg.streamlines\n",
    "\n",
    "# streamlines = dts.Streamlines(\n",
    "# dtu.transform_tracking_output(streamlines,\n",
    "#                               np.linalg.inv(img.affine)))\n",
    "\n",
    "# print(\"Segmenting fiber groups...\")\n",
    "# segmentation = seg.Segmentation()\n",
    "# segmentation.segment(bundles,\n",
    "#                      streamlines,\n",
    "#                      fdata=dti,\n",
    "#                      fbval=bval,\n",
    "#                      fbvec=bvec,\n",
    "#                      mapping=mapping,\n",
    "#                      reg_template=MNI_T2_img)\n",
    "\n",
    "\n",
    "# fiber_groups = segmentation.fiber_groups\n",
    "\n",
    "# print(\"Cleaning fiber groups...\")\n",
    "# for bundle in bundles:\n",
    "#     fiber_groups[bundle] = seg.clean_fiber_group(fiber_groups[bundle])\n",
    "\n",
    "# for kk in fiber_groups:\n",
    "#     print(kk, len(fiber_groups[kk]))\n",
    "\n",
    "#     sft = StatefulTractogram(\n",
    "#         dtu.transform_tracking_output(fiber_groups[kk], img.affine),\n",
    "#         img, Space.RASMM)\n",
    "\n",
    "#     save_tractogram(sft, origin + '/%s_afq.trk'%kk,\n",
    "#                     bbox_valid_check=False)\n",
    "\n",
    "# print(\"Extracting tract profiles...\")\n",
    "# for bundle in bundles:\n",
    "#     fig, ax = plt.subplots(1)\n",
    "#     weights = gaussian_weights(fiber_groups[bundle])\n",
    "#     profile = afq_profile(FA_data, fiber_groups[bundle],\n",
    "#                           np.eye(4), weights=weights)\n",
    "#     ax.plot(profile)\n",
    "#     ax.set_title(bundle)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (mscsd, datasink, [('wm_odf', '4_tract_Reconstruction.@par.@par.@par.@par'),\n",
    "#                                        ('gm_odf', '4_tract_Reconstruction.@par.@par.@par.@par.@par'),\n",
    "#                                        ('csf_odf','4_tract_Reconstruction.@par.@par.@par.@par.@par.@par')]),\n",
    "\n",
    "\n",
    "# dwi_flow = Workflow(name = 'dwi_flow')\n",
    "# dwi_flow.connect([(infosource, sf, [('subject_id','subject_id')]),\n",
    "#                   (sf, mrconv, [('dti', 'in_file')]),\n",
    "#                   (mrconv, dwi_res, [('out_file', 'in_file')]),\n",
    "#                   (sf, dwi_res, [('bval', 'in_bval'),\n",
    "#                                  ('bvec', 'in_bvec')]),\n",
    "#                   (dwi_res, datasink, [('csf_file', '4_DWI_Reconstruction.@par'),\n",
    "#                                       ('wm_file', '4_DWI_Reconstruction.@par.@par'),\n",
    "#                                       ('gm_file', '4_DWI_Reconstruction.@par.@par.@par')]),\n",
    "#                   (sf, ms_csd, [('dti','in_file'),\n",
    "#                                ('bval','in_bval'),\n",
    "#                                ('bvec','in_bvec'),\n",
    "#                                ('mask', 'mask_file')]),\n",
    "#                   (dwi_res, ms_csd, [('wm_file', 'wm_txt'),\n",
    "#                                      ('gm_file', 'gm_txt'),\n",
    "#                                      ('csf_file', 'csf_txt')]),\n",
    "#                   (ms_csd, datasink, [('wm_odf', '4_DWI_Reconstruction.@par.@par.@par.@par'),\n",
    "#                                      ('gm_odf','4_DWI_Reconstruction.@par.@par.@par.@par.@par'),\n",
    "#                                      ('csf_odf', '4_DWI_Reconstruction.@par.@par.@par.@par.@par.@par')]) \n",
    "#                  ])\n",
    "# dwi_flow.base_dir = workflow_dir\n",
    "# dwi_flow.write_graph(graph2use = 'flat')\n",
    "# dwi = dwi_flow.run('MultiProc', plugin_args={'n_procs': 4})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
